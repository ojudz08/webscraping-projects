{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e927b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17146373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing business world news article with beautifulsoup + multithreading\n",
    "# NOTE: Don't forget to modify this one such that if pages > 900, divide it by 2 or 3 to parse by batches\n",
    "\n",
    "def _parse_content(url):\n",
    "    headers = {'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'}\n",
    "    content = BeautifulSoup(requests.get(url, headers).content, \n",
    "                            'html.parser')\n",
    "    return content\n",
    "\n",
    "def _get_article_b4(inputParam):\n",
    "    \"\"\"helper function\"\"\"\n",
    "    dir_output = r'C:\\Users\\ojell\\Desktop\\Oj\\_Thesis\\Data\\news\\businessworld'\n",
    "    url = inputParam[0]\n",
    "    category = inputParam[1]\n",
    "    lstpage = inputParam[2]\n",
    "    ArtUrl = []\n",
    "    for pg in range(1, lstpage):\n",
    "        content = _parse_content(url + f\"page/{str(pg)}/\")\n",
    "        page = content.find('div', {'class': 'td-container td-category-container'})\n",
    "        next10 = page.find('div', {'class': 'td-ss-main-content'}).find_all('a', {'class': 'td-image-wrap'})\n",
    "        if pg % 50 == 0: time.sleep(10)\n",
    "        if pg == 1:\n",
    "            top5 = page.find('div', {'class': 'td-big-grid-wrapper'}).find_all('a', {'class': 'td-image-wrap'})\n",
    "            ArtUrl = ArtUrl + [item.get('href') for item in top5] + [item.get('href') for item in next10]\n",
    "        else:\n",
    "            ArtUrl = ArtUrl + [item.get('href') for item in next10]\n",
    "        dct = {\"Article Url\": ArtUrl, \"Category\": category}\n",
    "    df = pd.DataFrame.from_dict(dct)\n",
    "    filename = category.lower() + '.csv'\n",
    "    df.to_csv(os.path.join(dir_output, filename), index=False)\n",
    "    return df\n",
    "\n",
    "def par_main(news_url):\n",
    "    exclude = ['buying rates', 'foreign interest rates', 'philippine mutual funds',\n",
    "               'leaders and laggards', 'stock quotes', 'stock markets summary',\n",
    "               'non-bsp convertible currencies', 'bsp convertible currencies', 'us commodity futures',\n",
    "               'health',\n",
    "               'top stories', 'corporate','stock market', 'banking', 'economy',\n",
    "               'markets', 'the nation', 'world', 'opinion', 'infographics', \n",
    "               'b-side podcasts', 'sparkup', 'spotlight', 'labor', 'property', \n",
    "               'agribusiness', 'arts & leisure', 'technology', 'velocity',\n",
    "               'special features', 'sports', 'special reports']\n",
    "    \n",
    "    content = _parse_content(news_url)\n",
    "\n",
    "    catContent = content.find('ul', {'id': 'menu-header-menu-1'}).find_all('a')\n",
    "    category = [item.text.title() for item in catContent if item.text.lower() not in exclude]\n",
    "    category_url = [item.get('href') for item in catContent if item.text.lower() not in exclude]\n",
    "    \n",
    "    pagetab = [_parse_content(url).find('span', {'class': 'pages'}).text for url in category_url]\n",
    "    lstpg = [int(re.sub(r'[^\\w\\s]', '', page[-(len(page) - 10):])) for page in pagetab]\n",
    "    \n",
    "    output = 0\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as pool:\n",
    "        futures = [pool.submit(_get_article_b4, [category_url[i], category[i], lstpg[i]]) for i in range(0, 1)]\n",
    "        for f in concurrent.futures.as_completed(futures):\n",
    "            output += 1\n",
    "    return category\n",
    "\n",
    "newsWeb = 'https://www.bworldonline.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test = par_main(newsWeb)\n",
    "par_time = time.time() - start\n",
    "#print(f\"Parallel Output: {len(test)}\")\n",
    "print(f\"Parallel elapsed time: {round(par_time/60, 2)} mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a2b5e",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
